数据分析平台 - 后端开发执行手册 (v4.3 - 意图驱动版)
文档ID: DAP-BE-SDD-v4.3
密级: 内部机密
状态: 正式发布
核心架构: Supabase (PostgreSQL, 数据库视图, Edge Functions, pg_cron)
项目定位: 一个完全在 Supabase 生态内实现的、自动化的、能够根据前端指令动态执行深度分析的后端智能引擎。
第一阶段：数据建模层 - 构建商业智能“大脑” (预计 2-3 天)
目标： 在 Supabase 数据库中，基于现有的 raw_... 表，创建一系列高性能的、可直接用于深度分析的 SQL 视图。这是整个系统的“智能核心”。
任务 ID	✅	任务描述	详细执行步骤与 SQL 示例	验收标准
BE-1.1	☐	创建宏观健康度监控视图	1. 登录 Supabase Studio -> SQL Editor。<br>2. 编写并执行 SQL 创建一个视图，用于每日 KPI 的聚合与环比计算。<br> sql<br> CREATE OR REPLACE VIEW daily_kpi_summary_view AS<br> WITH daily_ga_metrics AS (<br> SELECT date, SUM(sessions) as total_sessions, SUM(total_users) as total_users<br> FROM raw_ga4_data -- 假设已有这些聚合指标<br> GROUP BY date<br> ),<br> daily_woo_metrics AS (<br> SELECT date(created_at) as date, COUNT(order_id) as total_orders, SUM(total) as total_revenue<br> FROM raw_woocommerce_orders<br> WHERE status IN ('completed', 'processing', 'on-hold')<br> GROUP BY date(created_at)<br> )<br> SELECT<br> COALESCE(ga.date, woo.date) as report_date,<br> COALESCE(ga.total_sessions, 0) AS total_sessions,<br> COALESCE(woo.total_orders, 0) AS total_orders,<br> COALESCE(woo.total_revenue, 0) AS total_revenue,<br> LAG(COALESCE(ga.total_sessions, 0), 1) OVER (ORDER BY COALESCE(ga.date, woo.date)) as prev_day_sessions,<br> LAG(COALESCE(woo.total_orders, 0), 7) OVER (ORDER BY COALESCE(ga.date, woo.date)) as prev_week_orders<br> FROM daily_ga_metrics ga<br> FULL OUTER JOIN daily_woo_metrics woo ON ga.date = woo.date; -- 使用 FULL OUTER JOIN 确保日期完整<br>	视图 daily_kpi_summary_view 创建成功。查询该视图能返回每日的核心指标及其日/周同比数据。
BE-1.2	☐	创建 UTM 渠道漏斗性能视图	1. 在 SQL Editor 中，编写并执行 SQL 创建一个视图，用于计算各渠道的完整转化漏斗。<br> sql<br> CREATE OR REPLACE VIEW utm_funnel_performance_view AS<br> WITH channel_sessions AS (<br> SELECT<br> (ga4_data ->> 'sessionSourceMedium') AS channel,<br> (ga4_data ->> 'sessionCampaignName') AS campaign,<br> (ga4_data ->> 'clientId') AS client_id,<br> date,<br> COUNT(*) as sessions<br> FROM raw_ga4_channel_data -- 使用渠道数据集<br> GROUP BY channel, campaign, client_id, date<br> ),<br> channel_orders AS (<br> SELECT<br> (SELECT value ->> 'value' FROM jsonb_array_elements(order_data -> 'meta_data') WHERE value ->> 'key' = '_ga_cid') as client_id,<br> COUNT(order_id) as orders,<br> SUM(total) as revenue<br> FROM raw_woocommerce_orders<br> WHERE status IN ('completed', 'processing', 'on-hold')<br> GROUP BY client_id<br> )<br> SELECT<br> cs.date as report_date, -- 添加日期维度以供筛选<br> cs.channel,<br> cs.campaign,<br> SUM(cs.sessions) as total_sessions,<br> COALESCE(SUM(co.orders), 0) as total_orders,<br> COALESCE(SUM(co.revenue), 0) as total_revenue,<br> CASE WHEN SUM(cs.sessions) > 0 THEN COALESCE(SUM(co.orders), 0)::float / SUM(cs.sessions) ELSE 0 END as conversion_rate<br> FROM channel_sessions cs<br> LEFT JOIN channel_orders co ON cs.client_id = co.client_id<br> GROUP BY cs.date, cs.channel, cs.campaign<br> ORDER BY total_revenue DESC;<br>	视图 utm_funnel_performance_view 创建成功，包含日期维度，可以按日期范围进行筛选。
BE-1.3	☐	创建页面优化分析视图	1. 在 SQL Editor 中，编写并执行 SQL 创建一个视图，用于聚合每个页面的流量、参与度和转化行为。<br> sql<br> CREATE OR REPLACE VIEW page_optimization_view AS<br> SELECT<br> date, -- 添加日期维度<br> (page_data ->> 'pagePath') AS page_path,<br> (page_data ->> 'deviceCategory') AS device,<br> SUM((page_data ->> 'sessions')::int) AS total_sessions,<br> AVG((page_data ->> 'averageSessionDuration')::float) AS avg_session_duration,<br> AVG((page_data ->> 'bounceRate')::float) AS bounce_rate,<br> SUM((page_data ->> 'addToCarts')::int) AS total_add_to_carts,<br> SUM((page_data ->> 'checkouts')::int) AS total_checkouts<br> FROM raw_ga4_page_behavior -- 使用页面行为数据集<br> GROUP BY date, page_path, device<br> ORDER BY total_sessions DESC;<br>	视图 page_optimization_view 创建成功，包含日期维度，可以按日期范围进行筛选。
BE-1.4	☐	为所有对象配置 RLS	1. 确认表 RLS: 确保所有 raw_... 表和 insights, recommendations 表已开启 RLS。<br>2. 为视图开启 RLS: 进入 Authentication -> Policies，为所有新创建的视图启用 RLS。<br>3. 创建读取策略: 为每个视图创建一个新策略，选择 "Enable read access for authenticated users"。	RLS 已在所有表和视图上开启。前端应用可以通过用户身份安全地查询这些分析视图。
第二阶段：核心后端逻辑开发 (Supabase Edge Functions)
目标： 构建并部署一系列职责清晰、安全可靠的云函数，作为系统的“执行手臂”。
任务 ID	✅	任务描述	详细执行步骤与代码/配置示例	验收标准
BE-2.1	☐	创建并部署数据同步函数	1. 本地开发: supabase functions new sync-external-data。<br>2. 编写同步逻辑: 在 index.ts 中，实现从 GSC, GA4, WooCommerce 拉取精要数据并写入 raw_... 表的逻辑。<br>3. 部署: supabase functions deploy sync-external-data。	sync-external-data 函数成功部署，调用后能将外部数据正确写入数据库。
BE-2.2	☐	创建智能分析引擎函数	1. 本地开发: supabase functions new generate-insight。<br>2. 编写代码: 在 index.ts 中，实现接收前端分析指令 (analysisType, dateRange, promptInstructions)，根据指令在后端查询对应视图，构建上下文，调用 Gemini API 并流式返回结果的完整逻辑。<br> (代码与我们上一版文档中的 BE-2.2 (最终版) 完全相同)<br>3. 部署: supabase functions deploy generate-insight。	generate-insight 函数成功部署。它现在是一个通用的 AI 分析引擎，能够基于前端提供的任何指令和筛选条件进行分析。
BE-2.3	☐	创建洞察保存函数	1. 本地开发: supabase functions new save-insight。<br>2. 编写写入逻辑: 这个函数接收前端解析好的结构化洞察（title, summary, recommendations），并使用 service_role_key 安全地将其写入 insights 和 recommendations 表。必须使用 Zod 或类似工具进行输入校验。<br> typescript<br> // supabase/functions/save-insight/index.ts (片段)<br> import { z } from 'https://deno.land/x/zod@v3.23.4/mod.ts'<br><br> const InsightSchema = z.object({<br> insight_title: z.string().min(5),<br> insight_summary: z.string().min(10),<br> recommendations: z.array(z.object({<br> description: z.string().min(5),<br> priority: z.enum(['High', 'Medium', 'Low'])<br> }))<br> });<br><br> // ... 在函数开始处 ...<br> const body = await req.json();<br> const parsedBody = InsightSchema.safeParse(body);<br> if (!parsedBody.success) {<br> return new Response(JSON.stringify({ error: 'Invalid request body', issues: parsedBody.error.issues }), { status: 400, ... });<br> }<br> const { insight_title, insight_summary, recommendations } = parsedBody.data;<br> // ... (Database insert logic) ...<br><br>3. 部署: supabase functions deploy save-insight。	save-insight 函数成功部署，并且具备了基本的输入校验能力，增强了系统的健壮性。

第三阶段：自动化、测试与运维 (完整版)
目标： 将后端系统从“能用”提升到“可靠”，建立一套完整的自动化、测试、部署和监控流程，确保其长期稳定运行。
任务 ID	✅	任务描述	详细执行步骤与技术细节	验收标准
BE-3.1	☐	实现自动化调度 (Cron Job)	1. 启用扩展: 在 Supabase Studio 中，进入 Database -> Extensions，启用 pg_cron。<br>2. 创建调度任务: 在 SQL Editor 中，运行 cron.schedule 命令，设置一个每天定时调用 sync-external-data 函数的作业。<br> sql<br> SELECT cron.schedule(<br> 'daily-data-sync-job', -- Unique job name<br> '0 2 * * *', -- Cron syntax for "at 2:00 AM UTC"<br> $$<br> SELECT net.http_post(<br> url:='https://<project-ref>.supabase.co/functions/v1/sync-external-data',<br> headers:='{"Authorization": "Bearer <service_role_key>"}'::jsonb,<br> body:='{}'::jsonb<br> ) AS request_id;<br> $$<br> );<br><br> 注意: service_role_key 可以在 Settings -> API 中找到。	pg_cron 任务创建成功。可以通过查询 cron.job 表看到任务详情。数据同步流程现在完全自动化。
BE-3.2	☐	为 Edge Functions 编写单元测试	1. 测试框架: Supabase Edge Functions 使用 Deno，我们可以使用 Deno 内置的测试运行器。<br>2. 创建测试文件: 在每个函数目录下（如 supabase/functions/sync-external-data/），创建一个 _test.ts 文件，例如 index_test.ts。<br>3. 编写测试用例: 使用 Deno.test 来定义测试。你需要使用桩 (Stubs) 和 模拟 (Mocks) 来模拟 Supabase 客户端和外部 API (fetch) 的行为，以避免在测试中发出真实的网络请求。<br> typescript<br> // supabase/functions/sync-external-data/index_test.ts<br> import { assertEquals } from "https://deno.land/std/testing/asserts.ts";<br> import { stub } from "https://deno.land/std/testing/mock.ts";<br> import * as supabaseJs from "https://esm.sh/@supabase/supabase-js@2";<br><br> Deno.test("sync-external-data should process and upsert data", async () => {<br> // 模拟 fetch 返回成功的 WooCommerce 数据<br> const fetchStub = stub(globalThis, "fetch", () => Promise.resolve(new Response(JSON.stringify([{id: 1, ...}]))));<br><br> // 模拟 Supabase 客户端的 upsert 方法<br> const upsertStub = stub(supabaseJs.SupabaseClient.prototype, "from", () => ({<br> upsert: () => Promise.resolve({ data: [{...}], error: null })<br> }));<br><br> // 在这里，你需要导入并调用你的函数处理逻辑<br> // const result = await handleRequest(new Request("http://localhost"));<br> // assertEquals(result.status, 200);<br><br> // 清理模拟<br> fetchStub.restore();<br> upsertStub.restore();<br> });<br><br>4. 运行测试: 在项目根目录，运行 supabase test --function <function-name>。	核心的 Edge Functions (sync-external-data, generate-insight, save-insight) 都有对应的单元测试。运行测试命令时，所有测试用例都能通过。
BE-3.3	☐	建立数据库迁移与版本控制流程	1. 本地开发: 在本地开发时，任何对数据库表结构或视图的修改，都应该先在本地 Supabase 环境（通过 supabase start 启动）中进行测试。<br>2. 生成迁移文件: 在对本地数据库进行了成功的修改后，运行 supabase db diff -f <migration_name> 来自动生成一个 SQL 迁移文件，该文件记录了你的所有变更。<br>3. 提交到 Git: 将生成的迁移文件（位于 supabase/migrations/ 目录下）提交到你的 Git 仓库。<br>4. 部署到生产: <br> - 手动部署 (推荐): 在部署新功能前，将迁移文件中的 SQL 复制到生产环境的 Supabase Studio 的 SQL Editor 中，并手动执行。<br> - CI/CD 部署 (高级): 在你的部署流水线（如 GitHub Actions）中，设置一个步骤来运行 supabase db push，这会自动将所有新的迁移应用到你的生产数据库。	所有数据库的结构变更都有对应的、版本化的 SQL 迁移文件记录在 Git 中。这使得数据库结构可追溯、可回滚，并且可以轻松地在新环境中重建。
BE-3.4	☐	建立 CI/CD 自动化流水线 (GitHub Actions)	1. 创建工作流文件: 在你的项目根目录下，创建 .github/workflows/supabase.yml。<br>2. 编写工作流: 定义一个在 push 到 main 分支时触发的工作流。<br> yaml<br> name: Deploy Supabase Functions<br><br> on:<br> push:<br> branches:<br> - main<br> paths:<br> - 'supabase/functions/**'<br><br> jobs:<br> deploy:<br> runs-on: ubuntu-latest<br> steps:<br> - uses: actions/checkout@v3<br> - uses: supabase/setup-cli@v1<br> with:<br> version: latest<br> - name: Deploy Supabase Functions<br> run: supabase functions deploy --project-ref ${{ secrets.SUPABASE_PROJECT_REF }}<br> env:<br> SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}<br><br>3. 配置 Secrets: 在你的 GitHub 仓库的 Settings -> Secrets and variables -> Actions 中，添加 SUPABASE_PROJECT_REF 和 SUPABASE_ACCESS_TOKEN (你的个人访问令牌)。	当你修改了 supabase/functions/ 目录下的任何代码并推送到 main 分支时，GitHub Actions 会自动、安全地将更新后的函数部署到你的 Supabase 项目。
BE-3.5	☐	建立监控与告警机制	1. 日志监控: 定期（例如每周一）检查 Supabase Studio 中 sync-external-data 的日志，确保 pg_cron 任务每天都成功执行。<br>2. 数据库性能监控: 在 Reports -> Database Health 中，关注 "CPU Usage", "RAM Usage" 和 "Disk I/O"。如果发现任何指标持续处于高位，说明你的 SQL 视图可能需要优化（例如添加索引）。<br>3. 设置告警 (推荐): 在 Supabase 的 Logs 页面，你可以根据查询过滤错误日志（例如 level = "error"），并将这个查询保存为一个 "Log Alert"。Supabase 会在新的错误日志出现时，通过邮件或 webhook 通知你。	你有了一套主动的监控机制。当自动化任务失败或数据库性能下降时，你能在第一时间收到通知，而不是等待用户报告问题。
完成这个完整的第三阶段后，你的后端系统将不仅功能强大、高度自动化，而且具备了生产级系统所必需的可测试性、可追溯性、自动化部署能力和主动监控能力。这确保了你的数据基座能够长期、稳定、可靠地为前端应用提供服务。